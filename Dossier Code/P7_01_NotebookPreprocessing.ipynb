{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d309b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4796d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "\n",
    "\n",
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "\n",
    "def do_sum(dataframe, group_cols, counted, agg_name):\n",
    "    gp = dataframe[group_cols + [counted]].groupby(group_cols)[counted].sum().reset_index().rename(columns={counted: agg_name})\n",
    "    dataframe = dataframe.merge(gp, on=group_cols, how='left')\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def group(df_to_agg, prefix, aggregations, aggregate_by= 'SK_ID_CURR'):\n",
    "    agg_df = df_to_agg.groupby(aggregate_by).agg(aggregations)\n",
    "    agg_df.columns = pd.Index(['{}{}_{}'.format(prefix, e[0], e[1].upper())\n",
    "                               for e in agg_df.columns.tolist()])\n",
    "    return agg_df.reset_index()\n",
    "\n",
    "\n",
    "def group_and_merge(df_to_agg, df_to_merge, prefix, aggregations, aggregate_by= 'SK_ID_CURR'):\n",
    "    agg_df = group(df_to_agg, prefix, aggregations, aggregate_by= aggregate_by)\n",
    "    return df_to_merge.merge(agg_df, how='left', on= aggregate_by)\n",
    "\n",
    "\n",
    "# Display/plot feature importance\n",
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lgbm_importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a9b8453",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Données/'\n",
    "nan_as_category = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f681c9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307511, 122)\n",
      "(48744, 121)\n",
      "(1716428, 17)\n",
      "(27299925, 3)\n",
      "(1670214, 37)\n",
      "(10001358, 8)\n",
      "(13605401, 8)\n",
      "(3840312, 23)\n"
     ]
    }
   ],
   "source": [
    "# Importation des jeux de données\n",
    "train_df = pd.read_csv(path + 'application_train.csv')\n",
    "print(train_df.shape)\n",
    "test_df = pd.read_csv(path + 'application_test.csv')\n",
    "print(test_df.shape)\n",
    "df_bureau = pd.read_csv(path + 'bureau.csv')\n",
    "print(df_bureau.shape)\n",
    "df_bureau_balance = pd.read_csv(path + 'bureau_balance.csv')\n",
    "print(df_bureau_balance.shape)\n",
    "df_previous = pd.read_csv(path + 'previous_application.csv')\n",
    "print(df_previous.shape)\n",
    "df_pos_cash = pd.read_csv(path + 'POS_CASH_balance.csv')\n",
    "print(df_pos_cash.shape)\n",
    "df_installments = pd.read_csv(path + 'installments_payments.csv')\n",
    "print(df_installments.shape)\n",
    "df_credit_card = pd.read_csv(path + 'credit_card_balance.csv')\n",
    "print(df_credit_card.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bfa3bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Application_Train\" final shape: (307506, 308)\n",
      "Process application_train - done in 3s\n"
     ]
    }
   ],
   "source": [
    "def get_age_label(days_birth):\n",
    "    \"\"\" Return the age group label (int). \"\"\"\n",
    "    age_years = -days_birth / 365\n",
    "    if age_years < 27: return 1\n",
    "    elif age_years < 40: return 2\n",
    "    elif age_years < 50: return 3\n",
    "    elif age_years < 65: return 4\n",
    "    elif age_years < 99: return 5\n",
    "    else: return 0\n",
    "\n",
    "# Preprocess application_train.csv and application_test.csv\n",
    "with timer(\"Process application_train\"):\n",
    "    df_applications = train_df\n",
    "    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n",
    "    df_applications = df_applications[df_applications['CODE_GENDER'] != 'XNA']\n",
    "    df_applications = df_applications[df_applications['AMT_INCOME_TOTAL'] < 20000000] # remove a outlier 117M\n",
    "    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n",
    "    df_applications['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
    "    df_applications['DAYS_LAST_PHONE_CHANGE'].replace(0, np.nan, inplace=True) # set null value\n",
    "\n",
    "    # Categorical features with Binary encode (0 or 1; two categories)\n",
    "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        df_applications[bin_feature], uniques = pd.factorize(df_applications[bin_feature])\n",
    "    # Categorical features with One-Hot encode\n",
    "    df_applications, cat_cols = one_hot_encoder(df_applications, nan_as_category)\n",
    "\n",
    "    # Flag_document features - count and kurtosis\n",
    "    docs = [f for f in df_applications.columns if 'FLAG_DOC' in f]\n",
    "    df_applications['DOCUMENT_COUNT'] = df_applications[docs].sum(axis=1)\n",
    "    df_applications['NEW_DOC_KURT'] = df_applications[docs].kurtosis(axis=1)\n",
    "\n",
    "    # Categorical age - based on target=1 plot\n",
    "    df_applications['AGE_RANGE'] = df_applications['DAYS_BIRTH'].apply(lambda x: get_age_label(x))\n",
    "\n",
    "    # New features based on External sources\n",
    "    df_applications['EXT_SOURCES_PROD'] = df_applications['EXT_SOURCE_1'] * df_applications['EXT_SOURCE_2'] * df_applications['EXT_SOURCE_3']\n",
    "    df_applications['EXT_SOURCES_WEIGHTED'] = df_applications.EXT_SOURCE_1 * 2 + df_applications.EXT_SOURCE_2 * 1 + df_applications.EXT_SOURCE_3 * 3\n",
    "    np.warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')\n",
    "    for function_name in ['min', 'max', 'mean', 'nanmedian', 'var']:\n",
    "        feature_name = 'EXT_SOURCES_{}'.format(function_name.upper())\n",
    "        df_applications[feature_name] = eval('np.{}'.format(function_name))(\n",
    "            df_applications[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']], axis=1)\n",
    "\n",
    "    # Some simple new features (percentages)\n",
    "    df_applications['DAYS_EMPLOYED_PERC'] = df_applications['DAYS_EMPLOYED'] / df_applications['DAYS_BIRTH']\n",
    "    df_applications['INCOME_CREDIT_PERC'] = df_applications['AMT_INCOME_TOTAL'] / df_applications['AMT_CREDIT']\n",
    "    df_applications['INCOME_PER_PERSON'] = df_applications['AMT_INCOME_TOTAL'] / df_applications['CNT_FAM_MEMBERS']\n",
    "    df_applications['ANNUITY_INCOME_PERC'] = df_applications['AMT_ANNUITY'] / df_applications['AMT_INCOME_TOTAL']\n",
    "    df_applications['PAYMENT_RATE'] = df_applications['AMT_ANNUITY'] / df_applications['AMT_CREDIT']\n",
    "\n",
    "    # Credit ratios\n",
    "    df_applications['CREDIT_TO_GOODS_RATIO'] = df_applications['AMT_CREDIT'] / df_applications['AMT_GOODS_PRICE']\n",
    "\n",
    "    # Income ratios\n",
    "    df_applications['INCOME_TO_EMPLOYED_RATIO'] = df_applications['AMT_INCOME_TOTAL'] / df_applications['DAYS_EMPLOYED']\n",
    "    df_applications['INCOME_TO_BIRTH_RATIO'] = df_applications['AMT_INCOME_TOTAL'] / df_applications['DAYS_BIRTH']\n",
    "\n",
    "    # Time ratios\n",
    "    df_applications['ID_TO_BIRTH_RATIO'] = df_applications['DAYS_ID_PUBLISH'] / df_applications['DAYS_BIRTH']\n",
    "    df_applications['CAR_TO_BIRTH_RATIO'] = df_applications['OWN_CAR_AGE'] / df_applications['DAYS_BIRTH']\n",
    "    df_applications['CAR_TO_EMPLOYED_RATIO'] = df_applications['OWN_CAR_AGE'] / df_applications['DAYS_EMPLOYED']\n",
    "    df_applications['PHONE_TO_BIRTH_RATIO'] = df_applications['DAYS_LAST_PHONE_CHANGE'] / df_applications['DAYS_BIRTH']\n",
    "\n",
    "    # EXT_SOURCE_X FEATURE\n",
    "    df_applications['APPS_EXT_SOURCE_MEAN'] = df_applications[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "    df_applications['APPS_EXT_SOURCE_STD'] = df_applications[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n",
    "    df_applications['APPS_EXT_SOURCE_STD'] = df_applications['APPS_EXT_SOURCE_STD'].fillna(df_applications['APPS_EXT_SOURCE_STD'].mean())\n",
    "    df_applications['APP_SCORE1_TO_BIRTH_RATIO'] = df_applications['EXT_SOURCE_1'] / (df_applications['DAYS_BIRTH'] / 365.25)\n",
    "    df_applications['APP_SCORE2_TO_BIRTH_RATIO'] = df_applications['EXT_SOURCE_2'] / (df_applications['DAYS_BIRTH'] / 365.25)\n",
    "    df_applications['APP_SCORE3_TO_BIRTH_RATIO'] = df_applications['EXT_SOURCE_3'] / (df_applications['DAYS_BIRTH'] / 365.25)\n",
    "    df_applications['APP_SCORE1_TO_EMPLOY_RATIO'] = df_applications['EXT_SOURCE_1'] / (df_applications['DAYS_EMPLOYED'] / 365.25)\n",
    "    df_applications['APP_EXT_SOURCE_2*EXT_SOURCE_3*DAYS_BIRTH'] = df_applications['EXT_SOURCE_1'] * df_applications['EXT_SOURCE_2'] * df_applications['DAYS_BIRTH']\n",
    "    df_applications['APP_SCORE1_TO_FAM_CNT_RATIO'] = df_applications['EXT_SOURCE_1'] / df_applications['CNT_FAM_MEMBERS']\n",
    "    df_applications['APP_SCORE1_TO_GOODS_RATIO'] = df_applications['EXT_SOURCE_1'] / df_applications['AMT_GOODS_PRICE']\n",
    "    df_applications['APP_SCORE1_TO_CREDIT_RATIO'] = df_applications['EXT_SOURCE_1'] / df_applications['AMT_CREDIT']\n",
    "    df_applications['APP_SCORE1_TO_SCORE2_RATIO'] = df_applications['EXT_SOURCE_1'] / df_applications['EXT_SOURCE_2']\n",
    "    df_applications['APP_SCORE1_TO_SCORE3_RATIO'] = df_applications['EXT_SOURCE_1'] / df_applications['EXT_SOURCE_3']\n",
    "    df_applications['APP_SCORE2_TO_CREDIT_RATIO'] = df_applications['EXT_SOURCE_2'] / df_applications['AMT_CREDIT']\n",
    "    df_applications['APP_SCORE2_TO_REGION_RATING_RATIO'] = df_applications['EXT_SOURCE_2'] / df_applications['REGION_RATING_CLIENT']\n",
    "    df_applications['APP_SCORE2_TO_CITY_RATING_RATIO'] = df_applications['EXT_SOURCE_2'] / df_applications['REGION_RATING_CLIENT_W_CITY']\n",
    "    df_applications['APP_SCORE2_TO_POP_RATIO'] = df_applications['EXT_SOURCE_2'] / df_applications['REGION_POPULATION_RELATIVE']\n",
    "    df_applications['APP_SCORE2_TO_PHONE_CHANGE_RATIO'] = df_applications['EXT_SOURCE_2'] / df_applications['DAYS_LAST_PHONE_CHANGE']\n",
    "    df_applications['APP_EXT_SOURCE_1*EXT_SOURCE_2'] = df_applications['EXT_SOURCE_1'] * df_applications['EXT_SOURCE_2']\n",
    "    df_applications['APP_EXT_SOURCE_1*EXT_SOURCE_3'] = df_applications['EXT_SOURCE_1'] * df_applications['EXT_SOURCE_3']\n",
    "    df_applications['APP_EXT_SOURCE_2*EXT_SOURCE_3'] = df_applications['EXT_SOURCE_2'] * df_applications['EXT_SOURCE_3']\n",
    "    df_applications['APP_EXT_SOURCE_1*DAYS_EMPLOYED'] = df_applications['EXT_SOURCE_1'] * df_applications['DAYS_EMPLOYED']\n",
    "    df_applications['APP_EXT_SOURCE_2*DAYS_EMPLOYED'] = df_applications['EXT_SOURCE_2'] * df_applications['DAYS_EMPLOYED']\n",
    "    df_applications['APP_EXT_SOURCE_3*DAYS_EMPLOYED'] = df_applications['EXT_SOURCE_3'] * df_applications['DAYS_EMPLOYED']\n",
    "\n",
    "    # AMT_INCOME_TOTAL : income\n",
    "    # CNT_FAM_MEMBERS  : the number of family members\n",
    "    df_applications['APPS_GOODS_INCOME_RATIO'] = df_applications['AMT_GOODS_PRICE'] / df_applications['AMT_INCOME_TOTAL']\n",
    "    df_applications['APPS_CNT_FAM_INCOME_RATIO'] = df_applications['AMT_INCOME_TOTAL'] / df_applications['CNT_FAM_MEMBERS']\n",
    "\n",
    "    # DAYS_BIRTH : Client's age in days at the time of application\n",
    "    # DAYS_EMPLOYED : How many days before the application the person started current employment\n",
    "    df_applications['APPS_INCOME_EMPLOYED_RATIO'] = df_applications['AMT_INCOME_TOTAL'] / df_applications['DAYS_EMPLOYED']\n",
    "\n",
    "    # other feature from better than 0.8\n",
    "    df_applications['CREDIT_TO_GOODS_RATIO_2'] = df_applications['AMT_CREDIT'] / df_applications['AMT_GOODS_PRICE']\n",
    "    df_applications['APP_AMT_INCOME_TOTAL_12_AMT_ANNUITY_ratio'] = df_applications['AMT_INCOME_TOTAL'] / 12. - df_applications['AMT_ANNUITY']\n",
    "    df_applications['APP_INCOME_TO_EMPLOYED_RATIO'] = df_applications['AMT_INCOME_TOTAL'] / df_applications['DAYS_EMPLOYED']\n",
    "    df_applications['APP_DAYS_LAST_PHONE_CHANGE_DAYS_EMPLOYED_ratio'] = df_applications['DAYS_LAST_PHONE_CHANGE'] / df_applications['DAYS_EMPLOYED']\n",
    "    df_applications['APP_DAYS_EMPLOYED_DAYS_BIRTH_diff'] = df_applications['DAYS_EMPLOYED'] - df_applications['DAYS_BIRTH']\n",
    "\n",
    "    print('\"Application_Train\" final shape:', df_applications.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "380e14a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Bureau/Bureau Balance\" final shape: (305811, 200)\n",
      "Process bureau and bureau_balance - done in 18s\n"
     ]
    }
   ],
   "source": [
    "# Preprocess bureau.csv and bureau_balance.csv\n",
    "with timer(\"Process bureau and bureau_balance\"):\n",
    "    # Credit duration and credit/account end date difference\n",
    "    df_bureau['CREDIT_DURATION'] = -df_bureau['DAYS_CREDIT'] + df_bureau['DAYS_CREDIT_ENDDATE']\n",
    "    df_bureau['ENDDATE_DIF'] = df_bureau['DAYS_CREDIT_ENDDATE'] - df_bureau['DAYS_ENDDATE_FACT']\n",
    "    \n",
    "    # Credit to debt ratio and difference\n",
    "    df_bureau['DEBT_PERCENTAGE'] = df_bureau['AMT_CREDIT_SUM'] / df_bureau['AMT_CREDIT_SUM_DEBT']\n",
    "    df_bureau['DEBT_CREDIT_DIFF'] = df_bureau['AMT_CREDIT_SUM'] - df_bureau['AMT_CREDIT_SUM_DEBT']\n",
    "    df_bureau['CREDIT_TO_ANNUITY_RATIO'] = df_bureau['AMT_CREDIT_SUM'] / df_bureau['AMT_ANNUITY']\n",
    "    df_bureau['BUREAU_CREDIT_FACT_DIFF'] = df_bureau['DAYS_CREDIT'] - df_bureau['DAYS_ENDDATE_FACT']\n",
    "    df_bureau['BUREAU_CREDIT_ENDDATE_DIFF'] = df_bureau['DAYS_CREDIT'] - df_bureau['DAYS_CREDIT_ENDDATE']\n",
    "    df_bureau['BUREAU_CREDIT_DEBT_RATIO'] = df_bureau['AMT_CREDIT_SUM_DEBT'] / df_bureau['AMT_CREDIT_SUM']\n",
    "\n",
    "    # CREDIT_DAY_OVERDUE :\n",
    "    df_bureau['BUREAU_IS_DPD'] = df_bureau['CREDIT_DAY_OVERDUE'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    df_bureau['BUREAU_IS_DPD_OVER120'] = df_bureau['CREDIT_DAY_OVERDUE'].apply(lambda x: 1 if x > 120 else 0)\n",
    "\n",
    "    df_bureau_balance, bb_cat = one_hot_encoder(df_bureau_balance, nan_as_category)\n",
    "    df_bureau, bureau_cat = one_hot_encoder(df_bureau, nan_as_category)\n",
    "\n",
    "    # Bureau balance: Perform aggregations and merge with bureau.csv\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size', 'mean']}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = ['mean']\n",
    "    # Status of Credit Bureau loan during the month\n",
    "    bb_agg = df_bureau_balance.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    df_bureau = df_bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "    \n",
    "    # Bureau and bureau_balance numeric features\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean', 'min'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean', 'max'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean', 'max', 'sum'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean', 'sum'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "        'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'],\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum'],\n",
    "        'SK_ID_BUREAU': ['count'],\n",
    "        'DAYS_ENDDATE_FACT': ['min', 'max', 'mean'],\n",
    "        'ENDDATE_DIF': ['min', 'max', 'mean'],\n",
    "        'BUREAU_CREDIT_FACT_DIFF': ['min', 'max', 'mean'],\n",
    "        'BUREAU_CREDIT_ENDDATE_DIFF': ['min', 'max', 'mean'],\n",
    "        'BUREAU_CREDIT_DEBT_RATIO': ['min', 'max', 'mean'],\n",
    "        'DEBT_CREDIT_DIFF': ['min', 'max', 'mean'],\n",
    "        'BUREAU_IS_DPD': ['mean', 'sum'],\n",
    "        'BUREAU_IS_DPD_OVER120': ['mean', 'sum']\n",
    "        }\n",
    "\n",
    "    # Bureau and bureau_balance categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "    df_bureau_agg = df_bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    df_bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in df_bureau_agg.columns.tolist()])\n",
    "\n",
    "    # Bureau: Active credits - using only numerical aggregations\n",
    "    active = df_bureau[df_bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    df_bureau_agg = df_bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    # Bureau: Closed credits - using only numerical aggregations\n",
    "    closed = df_bureau[df_bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    df_bureau_agg = df_bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    print('\"Bureau/Bureau Balance\" final shape:', df_bureau_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7f18e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous applications df shape: (338857, 321)\n",
      "Process previous_applications - done in 28s\n"
     ]
    }
   ],
   "source": [
    "# Preprocess previous_applications.csv\n",
    "with timer(\"Process previous_applications\"):\n",
    "    df_previous, cat_cols = one_hot_encoder(df_previous, nan_as_category=True)\n",
    "\n",
    "    # Days 365.243 values -> nan\n",
    "    df_previous['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace=True)\n",
    "    df_previous['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace=True)\n",
    "    df_previous['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace=True)\n",
    "    df_previous['DAYS_LAST_DUE'].replace(365243, np.nan, inplace=True)\n",
    "    df_previous['DAYS_TERMINATION'].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "    # Add feature: value ask / value received percentage\n",
    "    df_previous['APP_CREDIT_PERC'] = df_previous['AMT_APPLICATION'] / df_previous['AMT_CREDIT']\n",
    "\n",
    "    # Feature engineering: ratios and difference\n",
    "    df_previous['APPLICATION_CREDIT_DIFF'] = df_previous['AMT_APPLICATION'] - df_previous['AMT_CREDIT']\n",
    "    df_previous['CREDIT_TO_ANNUITY_RATIO'] = df_previous['AMT_CREDIT'] / df_previous['AMT_ANNUITY']\n",
    "    df_previous['DOWN_PAYMENT_TO_CREDIT'] = df_previous['AMT_DOWN_PAYMENT'] / df_previous['AMT_CREDIT']\n",
    "\n",
    "    # Interest ratio on previous application (simplified)\n",
    "    total_payment = df_previous['AMT_ANNUITY'] * df_previous['CNT_PAYMENT']\n",
    "    df_previous['SIMPLE_INTERESTS'] = (total_payment / df_previous['AMT_CREDIT'] - 1) / df_previous['CNT_PAYMENT']\n",
    "\n",
    "    # Days last due difference (scheduled x done)\n",
    "    df_previous['DAYS_LAST_DUE_DIFF'] = df_previous['DAYS_LAST_DUE_1ST_VERSION'] - df_previous['DAYS_LAST_DUE']\n",
    "\n",
    "    # from off\n",
    "    df_previous['PREV_GOODS_DIFF'] = df_previous['AMT_APPLICATION'] - df_previous['AMT_GOODS_PRICE']\n",
    "    df_previous['PREV_ANNUITY_APPL_RATIO'] = df_previous['AMT_ANNUITY'] / df_previous['AMT_APPLICATION']\n",
    "    df_previous['PREV_GOODS_APPL_RATIO'] = df_previous['AMT_GOODS_PRICE'] / df_previous['AMT_APPLICATION']\n",
    "\n",
    "    # Previous applications numeric features\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean', 'sum'],\n",
    "        'AMT_APPLICATION': ['min', 'max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean', 'sum'],\n",
    "        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'AMT_GOODS_PRICE': ['min', 'max', 'mean', 'sum'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "        'SK_ID_PREV': ['nunique'],\n",
    "        'DAYS_TERMINATION': ['max'],\n",
    "        'CREDIT_TO_ANNUITY_RATIO': ['mean', 'max'],\n",
    "        'APPLICATION_CREDIT_DIFF': ['min', 'max', 'mean', 'sum'],\n",
    "        'DOWN_PAYMENT_TO_CREDIT': ['mean'],\n",
    "        'PREV_GOODS_DIFF': ['mean', 'max', 'sum'],\n",
    "        'PREV_GOODS_APPL_RATIO': ['mean', 'max'],\n",
    "        'DAYS_LAST_DUE_DIFF': ['mean', 'max', 'sum'],\n",
    "        'SIMPLE_INTERESTS': ['mean', 'max']\n",
    "    }\n",
    "\n",
    "    # Previous applications categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "\n",
    "    df_previous_agg = df_previous.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    df_previous_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in df_previous_agg.columns.tolist()])\n",
    "\n",
    "    # Previous Applications: Approved Applications - only numerical features\n",
    "    approved = df_previous[df_previous['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "    df_previous_agg = df_previous_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    # Previous Applications: Refused Applications - only numerical features\n",
    "    refused = df_previous[df_previous['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "    df_previous_agg = df_previous_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "\n",
    "    print(\"Previous applications df shape:\", df_previous_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa08761d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Pos-Cash\" balance final shape: (337252, 46)\n",
      "Process POS-CASH balance - done in 109s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Process POS-CASH balance\"):\n",
    "    df_pos_cash, cat_cols = one_hot_encoder(df_pos_cash, nan_as_category=True)\n",
    "\n",
    "    # Flag months with late payment\n",
    "    df_pos_cash['LATE_PAYMENT'] = df_pos_cash['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    df_pos_cash['POS_IS_DPD'] = df_pos_cash['SK_DPD'].apply(lambda x: 1 if x > 0 else 0) # <-- same with ['LATE_PAYMENT']\n",
    "    df_pos_cash['POS_IS_DPD_UNDER_120'] = df_pos_cash['SK_DPD'].apply(lambda x: 1 if (x > 0) & (x < 120) else 0)\n",
    "    df_pos_cash['POS_IS_DPD_OVER_120'] = df_pos_cash['SK_DPD'].apply(lambda x: 1 if x >= 120 else 0)\n",
    "\n",
    "    # Features\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size', 'min'],\n",
    "        'SK_DPD': ['max', 'mean', 'sum', 'var', 'min'],\n",
    "        'SK_DPD_DEF': ['max', 'mean', 'sum'],\n",
    "        'SK_ID_PREV': ['nunique'],\n",
    "        'LATE_PAYMENT': ['mean'],\n",
    "        'SK_ID_CURR': ['count'],\n",
    "        'CNT_INSTALMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'CNT_INSTALMENT_FUTURE': ['min', 'max', 'mean', 'sum'],\n",
    "        'POS_IS_DPD': ['mean', 'sum'],\n",
    "        'POS_IS_DPD_UNDER_120': ['mean', 'sum'],\n",
    "        'POS_IS_DPD_OVER_120': ['mean', 'sum'],\n",
    "    }\n",
    "\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "\n",
    "    pos_agg = df_pos_cash.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "\n",
    "    # Count pos cash accounts\n",
    "    pos_agg['POS_COUNT'] = df_pos_cash.groupby('SK_ID_CURR').size()\n",
    "\n",
    "\n",
    "    sort_pos = df_pos_cash.sort_values(by=['SK_ID_PREV', 'MONTHS_BALANCE'])\n",
    "    gp = sort_pos.groupby('SK_ID_PREV')\n",
    "    df_pos = pd.DataFrame()\n",
    "    df_pos['SK_ID_CURR'] = gp['SK_ID_CURR'].first()\n",
    "    df_pos['MONTHS_BALANCE_MAX'] = gp['MONTHS_BALANCE'].max()\n",
    "\n",
    "    # Percentage of previous loans completed and completed before initial term\n",
    "    df_pos['POS_LOAN_COMPLETED_MEAN'] = gp['NAME_CONTRACT_STATUS_Completed'].mean()\n",
    "    df_pos['POS_COMPLETED_BEFORE_MEAN'] = gp['CNT_INSTALMENT'].first() - gp['CNT_INSTALMENT'].last()\n",
    "    df_pos['POS_COMPLETED_BEFORE_MEAN'] = df_pos.apply(lambda x: 1 if x['POS_COMPLETED_BEFORE_MEAN'] > 0 \\\n",
    "                                                                      and x['POS_LOAN_COMPLETED_MEAN'] > 0 else 0, axis=1)\n",
    "    # Number of remaining installments (future installments) and percentage from total\n",
    "    df_pos['POS_REMAINING_INSTALMENTS'] = gp['CNT_INSTALMENT_FUTURE'].last()\n",
    "    df_pos['POS_REMAINING_INSTALMENTS_RATIO'] = gp['CNT_INSTALMENT_FUTURE'].last()/gp['CNT_INSTALMENT'].last()\n",
    "\n",
    "    # Group by SK_ID_CURR and merge\n",
    "    df_gp = df_pos.groupby('SK_ID_CURR').sum().reset_index()\n",
    "    df_gp.drop(['MONTHS_BALANCE_MAX'], axis=1, inplace= True)\n",
    "    pos_agg = pd.merge(pos_agg, df_gp, on= 'SK_ID_CURR', how= 'left')\n",
    "\n",
    "    # Percentage of late payments for the 3 most recent applications\n",
    "    pos = do_sum(df_pos_cash, ['SK_ID_PREV'], 'LATE_PAYMENT', 'LATE_PAYMENT_SUM')\n",
    "\n",
    "    # Last month of each application\n",
    "    last_month_df = pos.groupby('SK_ID_PREV')['MONTHS_BALANCE'].idxmax()\n",
    "\n",
    "    # Most recent applications (last 3)\n",
    "    sort_pos = pos.sort_values(by=['SK_ID_PREV', 'MONTHS_BALANCE'])\n",
    "    gp = sort_pos.iloc[last_month_df].groupby('SK_ID_CURR').tail(3)\n",
    "    gp_mean = gp.groupby('SK_ID_CURR').mean().reset_index()\n",
    "    pos_agg = pd.merge(pos_agg, gp_mean[['SK_ID_CURR', 'LATE_PAYMENT_SUM']], on='SK_ID_CURR', how='left')\n",
    "\n",
    "    print('\"Pos-Cash\" balance final shape:', pos_agg.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fd7e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess installments_payments.csv\n",
    "# df_installments = pd.read_csv(path + 'installments_payments.csv')\n",
    "\n",
    "with timer(\"Process installments payments\"):\n",
    "    df_installments, cat_cols = one_hot_encoder(df_installments, nan_as_category=True)\n",
    "\n",
    "    # Group payments and get Payment difference\n",
    "    df_installments = do_sum(df_installments, ['SK_ID_PREV', 'NUM_INSTALMENT_NUMBER'], 'AMT_PAYMENT', 'AMT_PAYMENT_GROUPED')\n",
    "    df_installments['PAYMENT_DIFFERENCE'] = df_installments['AMT_INSTALMENT'] - df_installments['AMT_PAYMENT_GROUPED']\n",
    "    df_installments['PAYMENT_RATIO'] = df_installments['AMT_INSTALMENT'] / df_installments['AMT_PAYMENT_GROUPED']\n",
    "    df_installments['PAID_OVER_AMOUNT'] = df_installments['AMT_PAYMENT'] - df_installments['AMT_INSTALMENT']\n",
    "    df_installments['PAID_OVER'] = (df_installments['PAID_OVER_AMOUNT'] > 0).astype(int)\n",
    "\n",
    "    # Percentage and difference paid in each installment (amount paid and installment value)\n",
    "    df_installments['PAYMENT_PERC'] = df_installments['AMT_PAYMENT'] / df_installments['AMT_INSTALMENT']\n",
    "    df_installments['PAYMENT_DIFF'] = df_installments['AMT_INSTALMENT'] - df_installments['AMT_PAYMENT']\n",
    "\n",
    "    # Days past due and days before due (no negative values)\n",
    "    df_installments['DPD_diff'] = df_installments['DAYS_ENTRY_PAYMENT'] - df_installments['DAYS_INSTALMENT']\n",
    "    df_installments['DBD_diff'] = df_installments['DAYS_INSTALMENT'] - df_installments['DAYS_ENTRY_PAYMENT']\n",
    "    df_installments['DPD'] = df_installments['DPD_diff'].apply(lambda x: x if x > 0 else 0)\n",
    "    df_installments['DBD'] = df_installments['DBD_diff'].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "    # Flag late payment\n",
    "    df_installments['LATE_PAYMENT'] = df_installments['DBD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    df_installments['INSTALMENT_PAYMENT_RATIO'] = df_installments['AMT_PAYMENT'] / df_installments['AMT_INSTALMENT']\n",
    "    df_installments['LATE_PAYMENT_RATIO'] = df_installments.apply(lambda x: x['INSTALMENT_PAYMENT_RATIO'] if x['LATE_PAYMENT'] == 1 else 0, axis=1)\n",
    "\n",
    "    # Flag late payments that have a significant amount\n",
    "    df_installments['SIGNIFICANT_LATE_PAYMENT'] = df_installments['LATE_PAYMENT_RATIO'].apply(lambda x: 1 if x > 0.05 else 0)\n",
    "    \n",
    "    # Flag k threshold late payments\n",
    "    df_installments['DPD_7'] = df_installments['DPD'].apply(lambda x: 1 if x >= 7 else 0)\n",
    "    df_installments['DPD_15'] = df_installments['DPD'].apply(lambda x: 1 if x >= 15 else 0)\n",
    "\n",
    "    df_installments['INS_IS_DPD_UNDER_120'] = df_installments['DPD'].apply(lambda x: 1 if (x > 0) & (x < 120) else 0)\n",
    "    df_installments['INS_IS_DPD_OVER_120'] = df_installments['DPD'].apply(lambda x: 1 if (x >= 120) else 0)\n",
    "\n",
    "    # Features: Perform aggregations\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum', 'var'],\n",
    "        'DBD': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum', 'min'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum', 'min'],\n",
    "        'SK_ID_PREV': ['size', 'nunique'],\n",
    "        'PAYMENT_DIFFERENCE': ['mean'],\n",
    "        'PAYMENT_RATIO': ['mean', 'max'],\n",
    "        'LATE_PAYMENT': ['mean', 'sum'],\n",
    "        'SIGNIFICANT_LATE_PAYMENT': ['mean', 'sum'],\n",
    "        'LATE_PAYMENT_RATIO': ['mean'],\n",
    "        'DPD_7': ['mean'],\n",
    "        'DPD_15': ['mean'],\n",
    "        'PAID_OVER': ['mean'],\n",
    "        'DPD_diff':['mean', 'min', 'max'],\n",
    "        'DBD_diff':['mean', 'min', 'max'],\n",
    "        'DAYS_INSTALMENT': ['mean', 'max', 'sum'],\n",
    "        'INS_IS_DPD_UNDER_120': ['mean', 'sum'],\n",
    "        'INS_IS_DPD_OVER_120': ['mean', 'sum']\n",
    "    }\n",
    "\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    ins_agg = df_installments.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "\n",
    "    # Count installments accounts\n",
    "    ins_agg['INSTAL_COUNT'] = df_installments.groupby('SK_ID_CURR').size()\n",
    "\n",
    "    # from oof (DAYS_ENTRY_PAYMENT)\n",
    "    cond_day = df_installments['DAYS_ENTRY_PAYMENT'] >= -365\n",
    "    ins_d365_grp = df_installments[cond_day].groupby('SK_ID_CURR')\n",
    "    ins_d365_agg_dict = {\n",
    "        'SK_ID_CURR': ['count'],\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['mean', 'max', 'sum'],\n",
    "        'DAYS_INSTALMENT': ['mean', 'max', 'sum'],\n",
    "        'AMT_INSTALMENT': ['mean', 'max', 'sum'],\n",
    "        'AMT_PAYMENT': ['mean', 'max', 'sum'],\n",
    "        'PAYMENT_DIFF': ['mean', 'min', 'max', 'sum'],\n",
    "        'PAYMENT_PERC': ['mean', 'max'],\n",
    "        'DPD_diff': ['mean', 'min', 'max'],\n",
    "        'DPD': ['mean', 'sum'],\n",
    "        'INS_IS_DPD_UNDER_120': ['mean', 'sum'],\n",
    "        'INS_IS_DPD_OVER_120': ['mean', 'sum']}\n",
    "\n",
    "    ins_d365_agg = ins_d365_grp.agg(ins_d365_agg_dict)\n",
    "    ins_d365_agg.columns = ['INS_D365' + ('_').join(column).upper() for column in ins_d365_agg.columns.ravel()]\n",
    "\n",
    "    ins_agg = ins_agg.merge(ins_d365_agg, on='SK_ID_CURR', how='left')\n",
    "\n",
    "    print('\"Installments Payments\" final shape:', ins_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e743822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess credit_card_balance.csv\n",
    "with timer(\"Process credit card balance\"):\n",
    "    df_credit_card, cat_cols = one_hot_encoder(df_credit_card, nan_as_category=True)\n",
    "\n",
    "    # Amount used from limit\n",
    "    df_credit_card['LIMIT_USE'] = df_credit_card['AMT_BALANCE'] / df_credit_card['AMT_CREDIT_LIMIT_ACTUAL']\n",
    "    # Current payment / Min payment\n",
    "    df_credit_card['PAYMENT_DIV_MIN'] = df_credit_card['AMT_PAYMENT_CURRENT'] / df_credit_card['AMT_INST_MIN_REGULARITY']\n",
    "    # Late payment <-- 'CARD_IS_DPD'\n",
    "    df_credit_card['LATE_PAYMENT'] = df_credit_card['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    # How much drawing of limit\n",
    "    df_credit_card['DRAWING_LIMIT_RATIO'] = df_credit_card['AMT_DRAWINGS_ATM_CURRENT'] / df_credit_card['AMT_CREDIT_LIMIT_ACTUAL']\n",
    "\n",
    "    df_credit_card['CARD_IS_DPD_UNDER_120'] = df_credit_card['SK_DPD'].apply(lambda x: 1 if (x > 0) & (x < 120) else 0)\n",
    "    df_credit_card['CARD_IS_DPD_OVER_120'] = df_credit_card['SK_DPD'].apply(lambda x: 1 if x >= 120 else 0)\n",
    "\n",
    "    # General aggregations\n",
    "    cc_agg = df_credit_card.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "\n",
    "    # Count credit card lines\n",
    "    cc_agg['CC_COUNT'] = df_credit_card.groupby('SK_ID_CURR').size()\n",
    "\n",
    "    # Last month balance of each credit card application\n",
    "    last_ids = df_credit_card.groupby('SK_ID_PREV')['MONTHS_BALANCE'].idxmax()\n",
    "    last_months_df = df_credit_card[df_credit_card.index.isin(last_ids)]\n",
    "    cc_agg = group_and_merge(last_months_df, cc_agg, 'CC_LAST_', {'AMT_BALANCE': ['mean', 'max']})\n",
    "\n",
    "    CREDIT_CARD_TIME_AGG = {\n",
    "        'AMT_BALANCE': ['mean', 'max'],\n",
    "        'LIMIT_USE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_LIMIT_ACTUAL':['max'],\n",
    "        'AMT_DRAWINGS_ATM_CURRENT': ['max', 'sum'],\n",
    "        'AMT_DRAWINGS_CURRENT': ['max', 'sum'],\n",
    "        'AMT_DRAWINGS_POS_CURRENT': ['max', 'sum'],\n",
    "        'AMT_INST_MIN_REGULARITY': ['max', 'mean'],\n",
    "        'AMT_PAYMENT_TOTAL_CURRENT': ['max','sum'],\n",
    "        'AMT_TOTAL_RECEIVABLE': ['max', 'mean'],\n",
    "        'CNT_DRAWINGS_ATM_CURRENT': ['max','sum', 'mean'],\n",
    "        'CNT_DRAWINGS_CURRENT': ['max', 'mean', 'sum'],\n",
    "        'CNT_DRAWINGS_POS_CURRENT': ['mean'],\n",
    "        'SK_DPD': ['mean', 'max', 'sum'],\n",
    "        'LIMIT_USE': ['min', 'max'],\n",
    "        'DRAWING_LIMIT_RATIO': ['min', 'max'],\n",
    "        'LATE_PAYMENT': ['mean', 'sum'],\n",
    "        'CARD_IS_DPD_UNDER_120': ['mean', 'sum'],\n",
    "        'CARD_IS_DPD_OVER_120': ['mean', 'sum']\n",
    "    }\n",
    "\n",
    "    for months in [12, 24, 48]:\n",
    "        cc_prev_id = df_credit_card[df_credit_card['MONTHS_BALANCE'] >= -months]['SK_ID_PREV'].unique()\n",
    "        cc_recent = df_credit_card[df_credit_card['SK_ID_PREV'].isin(cc_prev_id)]\n",
    "        prefix = 'INS_{}M_'.format(months)\n",
    "        cc_agg = group_and_merge(cc_recent, cc_agg, prefix, CREDIT_CARD_TIME_AGG)\n",
    "\n",
    "    print('\"Credit Card Balance\" final shape:', cc_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55097604",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Applications df shape:\", df_applications.shape)\n",
    "print(\"Bureau df shape:\", df_bureau_agg.shape)\n",
    "df = df_applications.merge(df_bureau_agg, how='left', on='SK_ID_CURR')\n",
    "print(\"df shape after merge of Applications and Bureau :\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8bced6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Previous applications df shape:\", df_previous_agg.shape)\n",
    "df = df.merge(df_previous_agg, how='left', on='SK_ID_CURR')\n",
    "print(\"df shape after merge with Previous applications :\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510a2988",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pos-cash balance df shape:\", pos_agg.shape)\n",
    "df = df.merge(pos_agg, how='left', on='SK_ID_CURR')\n",
    "print(\"df shape after merge with Pos-cash balance :\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac54de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Installments payments df shape:\", ins_agg.shape)\n",
    "df = df.merge(ins_agg, how='left', on='SK_ID_CURR')\n",
    "print(\"df shape after merge with Installments payments :\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb883a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Credit card balance df shape:\", cc_agg.shape)\n",
    "df = df.merge(cc_agg, how='left', on='SK_ID_CURR')\n",
    "print(\"df shape after merge with Credit card :\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc42a161",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba997f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df_complet.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7963bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d19c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM GBDT with KFold or Stratified KFold\n",
    "# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code\n",
    "stratified = False\n",
    "debug = False\n",
    "num_folds = 10\n",
    "df = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "with timer(\"Run LightGBM with kfold\"):\n",
    "    # Divide in training/validation and test data\n",
    "    X = df.loc[:, df.columns != 'TARGET']\n",
    "    y = df['TARGET']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    train_df = X_train.join(y_train)\n",
    "    test_df = X_test.join(y_test)\n",
    "    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "    \n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=1001)\n",
    "    else:\n",
    "        folds = KFold(n_splits=num_folds, shuffle=True, random_state=1001)\n",
    "\n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "\n",
    "        # LightGBM parameters found by Bayesian optimization\n",
    "        clf = LGBMClassifier(\n",
    "            nthread=4,\n",
    "            n_estimators=10000,\n",
    "            learning_rate=0.02,\n",
    "            num_leaves=34,\n",
    "            colsample_bytree=0.9497036,\n",
    "            subsample=0.8715623,\n",
    "            max_depth=8,\n",
    "            reg_alpha=0.041545473,\n",
    "            reg_lambda=0.0735294,\n",
    "            min_split_gain=0.0222415,\n",
    "            min_child_weight=39.3259775,\n",
    "            silent=-1,\n",
    "            verbose=-1)\n",
    "\n",
    "        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "            eval_metric='auc', verbose=200, early_stopping_rounds=200)\n",
    "\n",
    "        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "        sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
    "\n",
    "    print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\n",
    "    display_importances(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2de79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = feature_importance_df[[\"feature\", \"importance\"]].groupby(\n",
    "    \"feature\").mean().sort_values(by=\"importance\", ascending=False)[:800].index\n",
    "best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "best_features.to_csv('best_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f41462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = pd.read_csv('best_features.csv')\n",
    "print(best_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d990cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "colstosave = best_features['feature'].unique()\n",
    "colonnes = []\n",
    "for col in df.columns:\n",
    "    if col in colstosave:\n",
    "        colonnes.append(col)\n",
    "colonnes.append('TARGET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dbf673",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df[colonnes]\n",
    "print(df_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485055b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('df_final.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
